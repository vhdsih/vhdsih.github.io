<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="《机器学习基石》系列课程（九） 我们一直在讨论分类问题，并利用分类问题推导了VC Bound以及Learning的可行性问题。Learning任务中也存在很多输出空间是连续的情形，实际上VC Bound同样使用在这些问题上，这一节讨论线性回归问题。">
<meta property="og:type" content="article">
<meta property="og:title" content="09 Linear Regression">
<meta property="og:url" content="https://vhdsih.github.io/2018/10/11/mlfound9/index.html">
<meta property="og:site_name" content="vhdsih">
<meta property="og:description" content="《机器学习基石》系列课程（九） 我们一直在讨论分类问题，并利用分类问题推导了VC Bound以及Learning的可行性问题。Learning任务中也存在很多输出空间是连续的情形，实际上VC Bound同样使用在这些问题上，这一节讨论线性回归问题。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/1.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/2.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/3.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/4.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/5.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/6.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/7.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/8.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/9.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/10.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/12.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/13.png">
<meta property="og:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/11.png">
<meta property="article:published_time" content="2018-10-11T14:06:54.000Z">
<meta property="article:modified_time" content="2021-12-10T07:41:11.478Z">
<meta property="article:author" content="vhdsih">
<meta property="article:tag" content="林轩田">
<meta property="article:tag" content="video-note">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vhdsih.github.io/2018/10/11/mlfound9/1.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>09 Linear Regression</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="vhdsih" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/archives/">Articles</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/atom.xml">Rss</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2018/10/12/mlfound10/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2018/10/11/mlfound8/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://vhdsih.github.io/2018/10/11/mlfound9/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://vhdsih.github.io/2018/10/11/mlfound9/&text=09 Linear Regression"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://vhdsih.github.io/2018/10/11/mlfound9/&is_video=false&description=09 Linear Regression"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=09 Linear Regression&body=Check out this article: https://vhdsih.github.io/2018/10/11/mlfound9/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://vhdsih.github.io/2018/10/11/mlfound9/&name=09 Linear Regression&description=&lt;p&gt;《机器学习基石》系列课程（九）&lt;/p&gt;
&lt;p&gt;我们一直在讨论分类问题，并利用分类问题推导了VC Bound以及Learning的可行性问题。Learning任务中也存在很多输出空间是连续的情形，实际上VC Bound同样使用在这些问题上，这一节讨论线性回归问题。&lt;/p&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://vhdsih.github.io/2018/10/11/mlfound9/&t=09 Linear Regression"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression-Problem"><span class="toc-number">1.</span> <span class="toc-text">Linear Regression Problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression-Algorithm"><span class="toc-number">2.</span> <span class="toc-text">Linear Regression Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generalization-Issue"><span class="toc-number">3.</span> <span class="toc-text">Generalization Issue</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression-for-Binary-Classification"><span class="toc-number">4.</span> <span class="toc-text">Linear Regression for Binary Classification</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        09 Linear Regression
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">vhdsih</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-10-11T14:06:54.000Z" class="dt-published" itemprop="datePublished">2018-10-11</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/video-note/" rel="tag">video-note</a>, <a class="p-category" href="/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/" rel="tag">林轩田</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>《机器学习基石》系列课程（九）</p>
<p>我们一直在讨论分类问题，并利用分类问题推导了VC Bound以及Learning的可行性问题。Learning任务中也存在很多输出空间是连续的情形，实际上VC Bound同样使用在这些问题上，这一节讨论线性回归问题。</p>
<span id="more"></span>
<h2 id="Linear-Regression-Problem"><a href="#Linear-Regression-Problem" class="headerlink" title="Linear Regression Problem"></a>Linear Regression Problem</h2><p>在我们讨论信用卡发放问题时，我们一直以来的输出都是是或者否的输出空间，如果我们在解决这个问题时，要求输出是一个人的信用程度，我们将根据这个信用程度来决定是否发放信用卡的时候，我们将要解决的问题就是回归问题。<br>那么我们的Hypothesis Set应该是什么样的，才能输出连续的内容呢？我们可以考虑为每一个维度的输入属性乘以权重，最终我们找到一个合适的权重向量来实现对信用的预测：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 设输入x为</span><br><span class="line">x = (x1, x2, x3, ..., xd)</span><br><span class="line"># 权重向量w为</span><br><span class="line">wT = (w1, w2, w3, ..., wd)</span><br><span class="line"># 那么</span><br><span class="line">y about= sum(wi * xi) (i from 0 to d)</span><br><span class="line"># 转换为向量运算：</span><br><span class="line">h(x) = wT * x</span><br></pre></td></tr></table></figure>
<p>上面的h(x)和Perceptron是很类似的，区别是后者有sign函数，将每个数值取为了正负1。<br>那么Linear Regression在空间中是什么样子的呢？在二维空间中，它是一条直线，而在三维（或高维）空间中，则是一个超平面。我们的Learning任务，实际就是在寻找这条（个）直线（超平面）。</p>
<p><img src="1.png" alt="see"> </p>
<p>在回归问题中，一般使用平方误差（Squared error）作为Error的衡量方法，即数据在Learning后输出的结果和实际结果差值的平方。这个方法可以同<br>时用在Ein和Eout上。Learning的过程就是最小化这个Squared Error的过程。</p>
<p><img src="2.png" alt="error"> </p>
<h2 id="Linear-Regression-Algorithm"><a href="#Linear-Regression-Algorithm" class="headerlink" title="Linear Regression Algorithm"></a>Linear Regression Algorithm</h2><p>在求解Regression问题时，实际是最小化Ein(w)。Ein是w的函数，它包含x和y两个向量参数，首先我们将其转化为矩阵表示：</p>
<p><img src="3.png" alt="matrix"> </p>
<p>Ein(w)实际上是连续的凸函数，也就意味着其有最小值，由高等数学可知，在最小值点，Ein(w)的梯度为0，我们找到了这一点的w：wlin，我们也就解决了问题。</p>
<p><img src="4.png" alt="show"> </p>
<p>我们现在求解Ein(w)的Gradient：∇Ein(w) ，将平方项展开，并将一些系数用简单的A、b、c表示：</p>
<p><img src="5.png" alt="∇Ein"> </p>
<p>求Ein的梯度，也就是求w的导数。如果w是一个变量，那么容易求解：</p>
<p><img src="6.png" alt="w is var"> </p>
<p>当w是一个vector时，实际上在求解之后和w是变量的时候具有相同的形式：</p>
<p><img src="7.png" alt="w is vector"> </p>
<p>我们现在的任务就变成了求Ein梯度为0时的w了：</p>
<p><img src="8.png" alt="task"> </p>
<p>如果X是可逆的就很容易，通过在等式两边乘以逆矩阵，我们就能够得到唯一的解(大部分情况都是可逆的，因为数据N远大于矩阵维度d+1，否则我们可以使用伪逆矩阵pseudo-inverse，很多软件都实现了这个方法，使用pseudo-inverse求解）。如果X不可逆，那么就会有很多解：</p>
<p><img src="9.png" alt="invertible or singular"> </p>
<p>我们用X+表示pseudo-inverse，那么最终通过计算这个伪逆矩阵，就能得到需要的权重向量：</p>
<p><img src="10.png" alt="invertible or singular"> </p>
<p>只要我们使用好的求解pseudo-inverse矩阵的方法，求解最优w的过程就很简单！</p>
<h2 id="Generalization-Issue"><a href="#Generalization-Issue" class="headerlink" title="Generalization Issue"></a>Generalization Issue</h2><p>上面的Linear Regression算法看起来很简单，可能有些人会产生疑问：这算是机器学习吗？因为他没有看到随着使用数据一步一步优化的过程。<br>实际上如果我们仔细推导了求解pseudo-iverse矩阵的过程，我们仍然能够看到这个逐步优化的过程，只不过现在它们都被封装好了，我们感觉很简单就计算出来了。所以我们仍然可以说这是一个Learning的问题：我们能够获得一个很小的Ein，由于问题有一个有限的VC维度，我们能够保证Eout和Ein大致相等。</p>
<p>事实上，我们可以利用一种比VC Dimension更简单的方法来证明这个问题：通过比较Ein的平均和Eout的平均。当然，这里只会提到其中重要的几个步骤（或思想）：</p>
<p>我们将求得的Wlin代入，来化简Ein：<br><img src="12.png" alt="average"> </p>
<p>我们将XX+称为帽子矩阵（hat matrix H），那么化简以后，Ein取决于帽子矩阵和y。<br>我们从几何学的角度来看这个问题：</p>
<p><img src="13.png" alt="hat matrix"> </p>
<p>现在我们假设我们待解决的问题是一个N维空间的问题：其中，ŷ都是我们求得的结果，而y是实际的结果。而令Ein最小化就是让y和ŷ最小。当前我们还不知道最好的w在哪，因为ŷ=Xw，那么我们将所有的w和X计算乘积以后会展开成一个超平面（上面粉色部分），我们让Ein最小只需要让ŷ是y在这个超平面的投影即可，当然我们还需要考虑ŷ的长度，但是很容易知道只要y-ŷ垂直与这个超平面的即可。那么帽子矩阵H就是把任何向量y都头应到这个超平面上，而I - H就是求y - ŷ。</p>
<p>如果我们计算（I-H）的迹 trace(I - H)，可以得到结果是N - (d + 1)，其物理意义是y-ŷ的自由度。</p>
<p>如果现在不是理想状态，而是包含Noise：</p>
<div aling=center> ![noise H](14.png) 

<p>此时：</p>
<div aling=center> ![ein](15.png) 

<p>我们就能得到Ein平均的情况，同理我们也能计算Eout的平均，只不过很复杂。</p>
<div aling=center> ![ein and eout](16.png) 

<p>我们此时可以画出Ein和Eout的平均的曲线：</p>
<div aling=center> ![curve](17.png) 

<p>所以，平均来说Eout和Ein的差是2 * (d + 1) / N。当N够大Learning是能够进行的。</p>
<h2 id="Linear-Regression-for-Binary-Classification"><a href="#Linear-Regression-for-Binary-Classification" class="headerlink" title="Linear Regression for Binary Classification"></a>Linear Regression for Binary Classification</h2><p>我们已经学习过线性分类问题，其表示形式和回归问题类似，区别在于其使用了sign函数，对于分类问题，我们的目标是找到一条最优的直线（分类超平面），然而解决这个问题往往是NP问题。<br>现在我们学习了线性回归分析，通过前面的求解，我们知道这是一种高效的方法。既然分类问题的输入空间是{-1, +1}，那么能不能使用回归方法来解决分类问题呢？<br>直观上看是可以的，对于-1的类别，我们的回归方法的输出可能是一个负数，或者说当输出是负数的时候我们将其看作是负类；相反，在输出正的时候可以看作正类。为了深入解释这个问题，我们现在观察两种问题的Error的不同。<br>对于分类问题，我们往往使用pointwise的方案，而回归问题一般是平方误差，这也就导致了前者在数学上的表现是一段段梯形的表示（不会平滑连续），而后者则是平滑的凸函数。</p>
<p><img src="11.png" alt="relation of errors"> </p>
<p>也就是说，分类的error总是小于或者等于回归的error！由此，我们可以使用Regression求得的最优权重向量wlin初始化PLA或者Pocket算法，然后再由PLA或Pocket进一步找到最优的解，这样就能很大程度上提高效率。</p>
<blockquote>
<p>文章内容和图片均来自“国立台湾大学林轩田老师”的《机器学习基石》课程！</p>
</blockquote>
<p>— END —</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/archives/">Articles</a></li>
        
          <li><a href="/tags/">Tag</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/atom.xml">Rss</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression-Problem"><span class="toc-number">1.</span> <span class="toc-text">Linear Regression Problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression-Algorithm"><span class="toc-number">2.</span> <span class="toc-text">Linear Regression Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generalization-Issue"><span class="toc-number">3.</span> <span class="toc-text">Generalization Issue</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression-for-Binary-Classification"><span class="toc-number">4.</span> <span class="toc-text">Linear Regression for Binary Classification</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://vhdsih.github.io/2018/10/11/mlfound9/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://vhdsih.github.io/2018/10/11/mlfound9/&text=09 Linear Regression"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://vhdsih.github.io/2018/10/11/mlfound9/&is_video=false&description=09 Linear Regression"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=09 Linear Regression&body=Check out this article: https://vhdsih.github.io/2018/10/11/mlfound9/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://vhdsih.github.io/2018/10/11/mlfound9/&title=09 Linear Regression"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://vhdsih.github.io/2018/10/11/mlfound9/&name=09 Linear Regression&description=&lt;p&gt;《机器学习基石》系列课程（九）&lt;/p&gt;
&lt;p&gt;我们一直在讨论分类问题，并利用分类问题推导了VC Bound以及Learning的可行性问题。Learning任务中也存在很多输出空间是连续的情形，实际上VC Bound同样使用在这些问题上，这一节讨论线性回归问题。&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://vhdsih.github.io/2018/10/11/mlfound9/&t=09 Linear Regression"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2023
    vhdsih
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/archives/">Articles</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/atom.xml">Rss</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
